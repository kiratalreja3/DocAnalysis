{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    # Construct the character 'vocabulary'\n",
    "    # we allow lowercase, uppercase and digits only, along with special characters:\n",
    "    # \"\" - Empty string used to denote elements for the RNN to ignore\n",
    "    # \"<bos>\" - Beginning of sequence token for the input the the RNN\n",
    "    # \".\" - End of sequence token\n",
    "    vocab = [\"\", \"<bos>\", \".\"] + list(string.ascii_lowercase + string.ascii_uppercase + string.digits + \" \")\n",
    "    id_to_char = {i: v for i, v in enumerate(vocab)} # maps from ids to characters\n",
    "    char_to_id = {v: i for i, v in enumerate(vocab)} # maps from characters to ids\n",
    "    return vocab, id_to_char, char_to_id\n",
    "\n",
    "def load_data(filename):\n",
    "    # read in the list of names\n",
    "    data = json.load(open(filename, \"r\"))\n",
    "    # append the end of sequence token to each name\n",
    "    data = [v+'.' for v in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqs_to_ids(seqs, char_to_id, max_len=20):\n",
    "    \"\"\"Takes a list of names and turns them into a list of tokens ids.\n",
    "    Responsible for padding sequences shorter than max_len with 0 so that all sequences are max_len.\n",
    "    Also truncates names that are longer than max_len.\n",
    "    Should also skip empty sequences if there are any.\n",
    "\n",
    "    Args:\n",
    "        seqs (list(str)): A list of names as strings.\n",
    "        char_to_id (dict(str : int)): The mapping for characters to token ids\n",
    "        max_len (int, optional): The maximum length of the ouput sequence. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        np.array: the names represented using token ids as 2d numpy array, \n",
    "            where each row corresponds to a name. The size of the array should be N * max_len\n",
    "            where N is the number of non-empty sequences input. Padded with zeros if needed.\n",
    "    \"\"\"\n",
    "    all_seqs = []\n",
    "\n",
    "    for name in seqs:\n",
    "        if len(name) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            temp_name = []\n",
    "            for onechar in name:\n",
    "                temp_name.append(char_to_id[onechar])\n",
    "            while len(temp_name) < max_len:\n",
    "                temp_name.append(0)\n",
    "            if len(temp_name) > max_len:\n",
    "                temp_name = temp_name[:20]\n",
    "        \n",
    "        all_seqs.append(temp_name)\n",
    "\n",
    "    # TODO: implement this function to turn a list of names into a 2d padded array of token ids\n",
    "\n",
    "    return np.array(all_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_seq(array,dict):\n",
    "    name = []\n",
    "    for place in array:\n",
    "        name.append(dict[place])\n",
    "\n",
    "    name = ''.join(name)\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size = 32, gru_size=32):\n",
    "        super(RNNLM, self).__init__()\n",
    "\n",
    "        # store layer sizes\n",
    "        self.emb_size = emb_size\n",
    "        self.gru_size = gru_size\n",
    "\n",
    "        # for embedding characters (ignores those with value 0: the padded values)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(emb_size, gru_size, batch_first=True)\n",
    "        # linear layer for output\n",
    "        self.linear = nn.Linear(gru_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, h_last=None):\n",
    "        \"\"\"Takes a batch of names/sequences expressed as token ids and passes them through the GRU.\n",
    "            The output is the predicted (un-normalized) probabilities of \n",
    "            the next token for all prefixes of the input sequences.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): A 2d tensor of longs giving the token ids for each batch. Shape B * S\n",
    "                where B is the batch size (any batch size >= 1 is permitted), S is the length of the sequence.\n",
    "            h_last (torch.tensor, optional): A 2d float tensor of size B * G where B is the batch size and G\n",
    "                is the dimensionality of the GRU hidden state. The hidden state from the previous step, provide only if \n",
    "                generating sequences iteratively. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple(torch.tensor, torch.tensor): first element of the tuple is the B * S * V where V is the vocabulary size.\n",
    "                This is the logit output of the RNNLM. The second element is the hidden state of the final step\n",
    "                of the GRU it should be B * G dimensional.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement this function which does the forward pass of the RNNLM network\n",
    "        embedded_input = self.emb(x)\n",
    "        if h_last is None:\n",
    "            h_last = torch.zeros(1,x.shape[0],32)\n",
    "        out,h_last = self.gru(embedded_input,h_last)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, h_last\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, y, batch_size):\n",
    "    mini_batches = []\n",
    "    data = np.hstack((X, y))\n",
    "    np.random.shuffle(data)\n",
    "    n_minibatches = data.shape[0] // batch_size\n",
    "    i = 0\n",
    "\n",
    "    for i in range(n_minibatches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
    "        X_mini = mini_batch[:, :20]\n",
    "        Y_mini = mini_batch[:, 20:]\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        mini_batch = data[i * batch_size:data.shape[0]]\n",
    "        X_mini = mini_batch[:, :20]\n",
    "        Y_mini = mini_batch[:, 20:]\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, Xtrain, Ytrain, Xval, Yval, id_to_char, max_epoch):\n",
    "    \"\"\"Train the RNNLM model using the Xtrain and Ytrain examples.\n",
    "    Uses mini-batch stochastic gradient descent with the Adam optimizer on \n",
    "    the mean cross entropy loss. Prints out the validation loss\n",
    "    after each epoch using calc_val_loss.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): the RNNLM model.\n",
    "        Xtrain (torch.tensor): The training data input sequence of size Nt * S. \n",
    "            Nt is the number of training examples, S is the sequence length. \n",
    "            The sequences always start with the <bos> token id.\n",
    "            The rest of the sequence is just Ytrain shifted to the right one position.\n",
    "            The sequence is zero padded.\n",
    "        Ytrain (torch.tensor): The expected output sequence of size Nt * S. \n",
    "            Does not start with the <bos> token.\n",
    "        Xval (torch.tensor): The validation data input sequence of size Nv * S. \n",
    "            Nv is the number of validation examples, S is the sequence length. \n",
    "            The sequences always start with the <bos> token id.\n",
    "            The rest of sequence is just Yval shifted to the right one position.\n",
    "            The sequence is zero padded.\n",
    "        Yval (torch.tensor): The expected output sequence for the validation data of size Nv * S. \n",
    "            Does not start with the <bos> token. Is zero padded.\n",
    "        id_to_char (dict(int : str)): A mapping from ids to tokens.\n",
    "        max_epoch (int): the maximum number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # construct the adam optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # construct the cross-entropy loss function\n",
    "    # we want to ignore padding cells with value == 0\n",
    "    lossfn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # calculate number of batches\n",
    "    batch_size = 32\n",
    "    num_batches = int(Xtrain.shape[0] / batch_size)\n",
    "    \n",
    "    \n",
    "\n",
    "    # run the main training loop over many epochs\n",
    "    for e in range(max_epoch):\n",
    "        \n",
    "        mini_batches = create_mini_batches(Xtrain,Ytrain,batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for current_batch in mini_batches:\n",
    "            runningloss = []\n",
    "            Xtrain_mini = current_batch[0]\n",
    "            Ytrain_mini = current_batch[1]\n",
    "            optim.zero_grad()\n",
    "            hidden = torch.zeros(1,Xtrain_mini.shape[0],32)\n",
    "            #output, hidden = model.forward(torch.from_numpy(Xtrain_mini),hidden)\n",
    "            output, hidden = model.forward(torch.from_numpy(Xtrain_mini),hidden)\n",
    "            #loss = lossfn(output.transpose(1,2),torch.from_numpy(Ytrain_mini))\n",
    "            loss = lossfn(output.permute(0,2,1),torch.from_numpy(Ytrain_mini))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            runningloss.append(loss.item())\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        print('Epoch: {}/{}.............'.format(e, max_epoch), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(sum(runningloss)/len(runningloss)))\n",
    "        print(\"Loss: {:.4f}\".format(calc_val_loss(model,Xval,Yval)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_val_loss(model, Xval, Yval):\n",
    "    \"\"\"Calculates the validation loss in average nats per character.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): the RNNLM model.\n",
    "        Xval (torch.tensor): The validation data input sequence of size B * S. \n",
    "            B is the batch size, S is the sequence length. The sequences always start with the <bos> token id.\n",
    "            The rest of sequence is just Yval shifted to the right one position.\n",
    "            The sequence is zero padded.\n",
    "        Yval (torch.tensor): The expected output sequence for the validation data of size B * S. \n",
    "            Does not start with the <bos> token. Is zero padded.\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss in average nats per character.\n",
    "    \"\"\"\n",
    "\n",
    "    # use cross entropy loss\n",
    "    lossfn = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # calculate number of batches, we need to be precise this time\n",
    "    batch_size = 32\n",
    "    num_batches = int(Xval.shape[0] / batch_size)\n",
    "    if Xval.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    # sum up the total loss\n",
    "    total_loss = 0\n",
    "    total_chars = 0\n",
    "    for n in range(num_batches):\n",
    "\n",
    "        # calculate batch start end idxs \n",
    "        s = n * batch_size\n",
    "        e = (n+1)*batch_size\n",
    "        if e > Xval.shape[0]:\n",
    "            e = Xval.shape[0]\n",
    "\n",
    "        # compute output of model        \n",
    "        out,_ = model(Xval[s:e])\n",
    "\n",
    "        # compute loss and store\n",
    "        loss = lossfn(out.permute(0, 2, 1), Yval[s:e]).detach().cpu().numpy()\n",
    "        total_loss += loss\n",
    "\n",
    "        char_count = torch.count_nonzero(Yval[s:e].flatten())\n",
    "        total_chars += char_count.detach().cpu().numpy()\n",
    "\n",
    "    # compute average loss per character\n",
    "    total_loss /= total_chars\n",
    "    \n",
    "    # set the model back to training mode in case we need gradients later\n",
    "    model.train()\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5............. Loss: 2.6970\n",
      "Loss: 2.4444\n",
      "Epoch: 1/5............. Loss: 2.1718\n",
      "Loss: 2.3823\n",
      "Epoch: 2/5............. Loss: 2.2649\n",
      "Loss: 2.3486\n",
      "Epoch: 3/5............. Loss: 2.1897\n",
      "Loss: 2.3295\n",
      "Epoch: 4/5............. Loss: 2.1429\n",
      "Loss: 2.3157\n"
     ]
    }
   ],
   "source": [
    "data = load_data(\"names_small.json\")\n",
    "\n",
    "# get the letter 'vocabulary'\n",
    "vocab, id_to_char, char_to_id = get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# convert the data into a sequence of ids which will be the target for our RNN\n",
    "Y = seqs_to_ids(data, char_to_id)\n",
    "# the input needs to be shifted by 1 and have the <bos> tokenid prepended to it\n",
    "# this also means we have to remove the last element of the sequence to keep the length constant\n",
    "X = np.concatenate([np.ones((Y.shape[0], 1)), Y[:, :-1]], axis=1)\n",
    "\n",
    "# split the data int training and validation\n",
    " # convert the data into torch tensors\n",
    "train_frac = 0.9\n",
    "num_train = int(X.shape[0]*train_frac)\n",
    "Xtrain = torch.tensor(X[:num_train], dtype=torch.long)\n",
    "Ytrain = torch.tensor(Y[:num_train], dtype=torch.long)\n",
    "Xval = torch.tensor(X[num_train:], dtype=torch.long)\n",
    "Yval = torch.tensor(Y[num_train:], dtype=torch.long)\n",
    "\n",
    "model = RNNLM(vocab_size)\n",
    "train_model(model, Xtrain, Ytrain, Xval, Yval, id_to_char, max_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_string(model, id_to_char, max_len=20, sample=True):\n",
    "    \"\"\"Generate a name using the model. The generation process should finish once\n",
    "    the end token is seen. We either sample from the model, where the next token is\n",
    "    chosen randomly according to the categorical probability distribution produced by softmax,\n",
    "    or we use argmax decoding where the most likely token is chosen at every generation step.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): The trained RNNLM model.\n",
    "        id_to_char (dict(int, str)): A mapping from token ids to token strings.\n",
    "        max_len (int, optional): The maximum length of the output sequence. Defaults to 20.\n",
    "        sample (bool, optional): If True then generate samples. If False then use argmax decoding. \n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name as a string.\n",
    "    \"\"\"\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # setup the initial input to the network\n",
    "    # we will use a batch size of one for generation\n",
    "    h = torch.zeros((1,1,model.gru_size), dtype=torch.float) # h0 is all zeros\n",
    "    x = torch.ones((1, 1), dtype=torch.long) # x is the <bos> token id which = 1\n",
    "    out_str = \"\"\n",
    "    stringlist = []\n",
    "    # generate the sequence step by step\n",
    "    for i in range(max_len):\n",
    "\n",
    "        # TODO: implement the generation loop of the RNNLM model\n",
    "        #       this should generate a name from the model\n",
    "        #       using either sampling or argmax decoding \n",
    "\n",
    "        out, hidden = model.forward(x,h)\n",
    "        prob = nn.functional.softmax(out, dim=2).data\n",
    "        char_ind = torch.max(prob, dim=2)[1].item()\n",
    "        stringlist.append(id_to_char[char_ind])\n",
    "        h = hidden\n",
    "        \n",
    "\n",
    "        pass\n",
    "\n",
    "    out_str = \"\".join(stringlist)\n",
    "\n",
    "    # set the model back to training mode in case we need gradients later\n",
    "    model.train()\n",
    "\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JMMMMMMMMMMMMMMMMMMM'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_string(model,id_to_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('c310dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdab2fb02dc4d68ed9e9d327e285305822cd116f79900361a665d6985eb56184"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
